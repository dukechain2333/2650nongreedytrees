# Basics of Decision Trees

Decision trees are hierarchical models used in machine learning for decision-making tasks. They represent decisions and their possible consequences in a tree-like structure, where each internal node represents a decision based on a feature, each branch represents the outcome of that decision, and each leaf node represents a class label or a continuous value. Decision trees are intuitive and easy to interpret, making them valuable for both classification and regression tasks. They recursively partition the input space into subsets based on the features' values, aiming to maximize the homogeneity of the target variable within each subset. This process continues until a stopping criterion is met, resulting in a tree that can be used to make predictions on new data by following the path from the root to a leaf node. Additionally, decision trees can handle both categorical and numerical data, making them a suitable tool for various applications. Decision trees and forests are widely utilized in machine learning and have gained increased popularity due to their computational efficiency and suitability for large-scale classification and regression tasks.

Greedy algorithms for decision tree construction optimize split functions one node at a time based on predefined criteria. However, this strategy often results in suboptimal trees. In this paper, the research introduces an algorithm designed to address the inherent limitation of such greedy algorithms, wherein the split function 
 lacks refinement based on outcomes from training at lower levels of the tree. The proposed algorithm guided by a global objective optimizes split functions across all levels of the tree. The paper demonstrates that the task of finding optimal linear-combination splits for decision trees is similar to structured prediction with latent variables, and proposes a convex-concave upper bound on the tree's empirical loss. This bound acts as a surrogate objective, optimized via stochastic gradient descent (SGD) to determine a locally optimal arrangement of the split functions. While conventional algorithms for decision tree induction remain greedy, the paperâ€™s proposed framework offers a non-greedy approach to learning split parameters, addressing some limitations of greedy approaches. By jointly optimizing split functions across different levels of the tree, their algorithm promotes collaboration among split nodes, resulting in more concise trees and improved generalization performance.
